{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e389e97",
   "metadata": {},
   "source": [
    "### Erwin Antepuesto \n",
    "BSCS - 3<br>\n",
    "Dec 2, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9553616c",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "1. The prefinals can be done at home.\n",
    "2. The Deadline will be on Saturday (Dec 2, 2023) at exactly 11:59 pm (no extensions will be given)\n",
    "3. Dataset: https://archive.ics.uci.edu/dataset/830/visegrad+group+companies+data-2\n",
    "4. Program from scratch Principal Component Analysis and Singular Value Decomposition.\n",
    "5. Implement your works with the given dataset using jupyterhub. Compare your result with that of the sklearn library in python. Explain why your results are similar or dissimilar and cite which specific part of your code causes the deviation of result.\n",
    "6. Push your .ipynb to your github account.\n",
    "7. Along with the necessary information, copy paste your github link here: https://docs.google.com/spreadsheets/d/1FdyiTSDGZbkf-VpIn2n_XHjUDzzBjj97fk4t20DWl3Y/edit?usp=sharing\n",
    "\n",
    "### Guidelines:\n",
    "1. no libraries at all, and just using raw python: no library should be used.\n",
    "2. minimal libraries are allowed, such as those for plotting, if this project involves plotting things into graphs: no plotting involved in this, matching between your solution and the one obtained from sklearn can be done programmatically.\n",
    "3. any library is allowed as long as it's not sklearn: no library should be use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ec822",
   "metadata": {},
   "source": [
    "## Part 1  \n",
    "### Dataset Imports and Preprocessing for Own Solution and <i>sklearn</i> Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e17d9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file paths\n",
    "file_paths = [\n",
    "    r'C:\\Users\\ACER\\Documents\\Python\\cs3101-prefinals\\dataset\\2017.arff',\n",
    "    r'C:\\Users\\ACER\\Documents\\Python\\cs3101-prefinals\\dataset\\2018.arff',\n",
    "    r'C:\\Users\\ACER\\Documents\\Python\\cs3101-prefinals\\dataset\\2019.arff',\n",
    "    r'C:\\Users\\ACER\\Documents\\Python\\cs3101-prefinals\\dataset\\2020.arff',\n",
    "    r'C:\\Users\\ACER\\Documents\\Python\\cs3101-prefinals\\dataset\\2021 Q1.arff',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a39fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cmath\n",
    "\n",
    "def load_and_preprocess_data(file_paths):\n",
    "    # Load datasets into a list of DataFrames\n",
    "    dfs = [pd.DataFrame(list(arff.load(file_path))) for file_path in file_paths]\n",
    "\n",
    "    # Concatenate DataFrames along rows\n",
    "    merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = merged_df.isnull().sum()\n",
    "\n",
    "    # Handle missing values (example: drop rows with any missing values)\n",
    "    merged_df = merged_df.dropna()\n",
    "\n",
    "    # Apply LabelEncoder to object columns after converting values to strings\n",
    "    le = LabelEncoder()\n",
    "    for column in merged_df.columns:\n",
    "        if merged_df[column].dtype == 'object':\n",
    "            merged_df[column] = merged_df[column].astype(str)\n",
    "            merged_df[column] = le.fit_transform(merged_df[column])\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec59d2",
   "metadata": {},
   "source": [
    "## Part 2  \n",
    "### Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) using <i>sklearn</i> Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b5cb3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Results:\n",
      "Explained Variance Ratio (PCA):\t [0.4172533  0.08504119]\n",
      "Eigenvalues (PCA):\t\t\t [1405229.4284625   286402.49043666]\n",
      "Cumulative Explained Variance (PCA):\t [0.4172533 0.5022945]\n",
      "\n",
      "SVD Results:\n",
      "Explained Variance Ratio (SVD):\t [0.36837609 0.08561444]\n",
      "Singular Values (SVD):\t\t\t [133109.6694485   26344.70205895]\n",
      "Cumulative Explained Variance (SVD):\t [0.36837609 0.45399053]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "data = load_and_preprocess_data(file_paths)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents_pca = pca.fit_transform(data)\n",
    "\n",
    "# Apply Truncated SVD\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "principalComponents_svd = svd.fit_transform(data)\n",
    "\n",
    "# Display the output\n",
    "# PCA\n",
    "explained_variance_ratio_pca = pca.explained_variance_ratio_\n",
    "eigenvalues_pca = pca.explained_variance_\n",
    "cumulative_explained_variance_pca = explained_variance_ratio_pca.cumsum()\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(\"Explained Variance Ratio (PCA):\\t\", explained_variance_ratio_pca)\n",
    "print(\"Eigenvalues (PCA):\\t\\t\\t\", eigenvalues_pca)\n",
    "print(\"Cumulative Explained Variance (PCA):\\t\", cumulative_explained_variance_pca)\n",
    "\n",
    "# SVD\n",
    "explained_variance_ratio_svd = svd.explained_variance_ratio_\n",
    "singular_values_svd = svd.singular_values_\n",
    "cumulative_explained_variance_svd = explained_variance_ratio_svd.cumsum()\n",
    "\n",
    "print(\"\\nSVD Results:\")\n",
    "print(\"Explained Variance Ratio (SVD):\\t\", explained_variance_ratio_svd)\n",
    "print(\"Singular Values (SVD):\\t\\t\\t\", singular_values_svd)\n",
    "print(\"Cumulative Explained Variance (SVD):\\t\", cumulative_explained_variance_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875ad68",
   "metadata": {},
   "source": [
    "## Part 3.1  \n",
    "### Principal Component Analysis (PCA) Manual Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6a47a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom PCA Results:\n",
      "Explained Variance Ratio:\t [0.5591340500663632, 0.4408659499336368]\n",
      "Eigenvalues:\t\t\t [170926974.74379313, 134772480.91179776]\n",
      "Cumulative Explained Variance:\t [0.5591340500663632, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def normalize(vector):\n",
    "    norm = sum(x**2 for x in vector)**0.5\n",
    "    return [x / norm for x in vector]\n",
    "\n",
    "def pca(data, num_components=2):\n",
    "    # Calculate means for each column\n",
    "    means = [mean(column) for column in zip(*data)]\n",
    "\n",
    "    # Center the data by subtracting means\n",
    "    centered_data = [[x - mean for x, mean in zip(row, means)] for row in data]\n",
    "\n",
    "    # Calculate covariance matrix\n",
    "    cov_matrix = [[covariance(col_x, col_y, mean_x, mean_y)\n",
    "                   for col_y, mean_y in zip(centered_data, means)]\n",
    "                  for col_x, mean_x in zip(centered_data, means)]\n",
    "\n",
    "    # Calculate eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = [], []\n",
    "    for _ in range(num_components):\n",
    "        # Power iteration method with normalization\n",
    "        eigenvector = [1.0] * len(cov_matrix)\n",
    "        for _ in range(100):\n",
    "            next_eigenvector = [sum(cov * x for cov, x in zip(row, eigenvector))\n",
    "                                for row in cov_matrix]\n",
    "            # Normalize the eigenvector\n",
    "            eigenvector = normalize(next_eigenvector)\n",
    "\n",
    "        # Eigenvalue calculation\n",
    "        eigenvalue = sum(row[i] * eigenvector[i] for i, row in enumerate(cov_matrix))\n",
    "\n",
    "        eigenvalues.append(abs(eigenvalue))\n",
    "        eigenvectors.append(eigenvector)\n",
    "\n",
    "        # Deflate the covariance matrix\n",
    "        for i in range(len(cov_matrix)):\n",
    "            for j in range(len(cov_matrix)):\n",
    "                cov_matrix[i][j] -= eigenvalue * eigenvector[i] * eigenvector[j]\n",
    "\n",
    "    # Sort eigenvalues and corresponding eigenvectors in descending order\n",
    "    sorted_indices = sorted(range(len(eigenvalues)), key=lambda k: eigenvalues[k], reverse=True)\n",
    "    eigenvalues = [eigenvalues[i] for i in sorted_indices]\n",
    "    eigenvectors = [eigenvectors[i] for i in sorted_indices]\n",
    "\n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "# Example usage:\n",
    "data_values = data.values  # Assuming 'data' is your DataFrame\n",
    "eigenvalues_custom, eigenvectors_custom = pca(data_values)\n",
    "\n",
    "# Display the output\n",
    "explained_variance_ratio_custom = [ev / sum(eigenvalues_custom) for ev in eigenvalues_custom]\n",
    "cumulative_explained_variance_custom = [sum(explained_variance_ratio_custom[:i+1])\n",
    "                                        for i in range(len(explained_variance_ratio_custom))]\n",
    "\n",
    "print(\"Custom PCA Results:\")\n",
    "print(\"Explained Variance Ratio:\\t\", explained_variance_ratio_custom)\n",
    "print(\"Eigenvalues:\\t\\t\\t\", eigenvalues_custom)\n",
    "print(\"Cumulative Explained Variance:\\t\", cumulative_explained_variance_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afb9c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Results:\n",
    "#Explained Variance Ratio (PCA):\t [0.4172533  0.08504119]\n",
    "#Eigenvalues (PCA):\t\t\t [1405229.4284625   286402.49043675]\n",
    "#Cumulative Explained Variance (PCA):\t [0.4172533 0.5022945]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d1e10",
   "metadata": {},
   "source": [
    "## Part 3.2  \n",
    "### Singular Value Decomposition (SVD) Manual Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1462afaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manual SVD Results:\n",
      "Explained Variance Ratio (SVD):\t [0.782912432611862, 0.041141589703449395]\n",
      "Singular Values (Manual SVD):\t\t [(23633.322046648704+0j), (5417.622156598941+0j)]\n",
      "Cumulative Explained Variance (SVD):\t [0.782912432611862, 0.8240540223153114]\n"
     ]
    }
   ],
   "source": [
    "data = load_and_preprocess_data(file_paths)\n",
    "\n",
    "def manual_svd(matrix):\n",
    "    # Calculate A^T * A\n",
    "    rows, cols = len(matrix), len(matrix[0])\n",
    "    ata = [[sum(matrix[i][k] * matrix[j][k] for k in range(cols)) for j in range(cols)] for i in range(cols)]\n",
    "\n",
    "    # Calculate A * A^T\n",
    "    aat = [[sum(matrix[i][k] * matrix[i][j] for i in range(cols)) for j in range(cols)] for k in range(cols)]\n",
    "\n",
    "    # Eigenvalue decomposition for A^T * A\n",
    "    eigenvalues_ata, eigenvectors_ata = eigenvalue_decomposition(ata)\n",
    "\n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = sorted(range(len(eigenvalues_ata)), key=lambda k: eigenvalues_ata[k], reverse=True)\n",
    "    eigenvalues_ata = [eigenvalues_ata[i] for i in idx]\n",
    "    eigenvectors_ata = [[eigenvectors_ata[j][i] for j in range(len(eigenvectors_ata))] for i in idx]\n",
    "\n",
    "    # Calculate singular values and sort them\n",
    "    singular_values = [cmath.sqrt(value) for value in eigenvalues_ata]\n",
    "\n",
    "    # Calculate matrix U\n",
    "    u = eigenvectors_ata\n",
    "\n",
    "    # Calculate matrix V\n",
    "    v = [[sum(u[j][i] * matrix[k][i] for i in range(cols)).real / singular_values[j] for j in range(cols)] for k in range(rows)]\n",
    "\n",
    "    return u, singular_values, v\n",
    "\n",
    "# Apply manual SVD\n",
    "u_manual, singular_values_manual, v_manual = manual_svd(data.values.tolist())\n",
    "\n",
    "# Sort singular values in descending order\n",
    "idx_sort = sorted(range(len(singular_values_manual)), key=lambda k: singular_values_manual[k].real, reverse=True)\n",
    "singular_values_manual = [singular_values_manual[i] for i in idx_sort]\n",
    "u_manual = [[u_manual[j][i] for j in range(len(u_manual))] for i in idx_sort]\n",
    "v_manual = [[v_manual[j][i] for j in range(len(v_manual))] for i in idx_sort]\n",
    "\n",
    "# Calculate explained variance ratio for manual SVD\n",
    "explained_variance_ratio_manual = [(singular_value.real ** 2) / sum(sv.real ** 2 for sv in singular_values_manual) for singular_value in singular_values_manual]\n",
    "\n",
    "# Calculate cumulative explained variance for manual SVD\n",
    "cumulative_explained_variance_manual = [sum(explained_variance_ratio_manual[:i + 1]) for i in range(len(explained_variance_ratio_manual))]\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nManual SVD Results:\")\n",
    "print(\"Explained Variance Ratio (SVD):\\t\", explained_variance_ratio_manual[:2])\n",
    "print(\"Singular Values (Manual SVD):\\t\\t\", singular_values_manual[:2])  # Display only the first two singular values for comparison\n",
    "print(\"Cumulative Explained Variance (SVD):\\t\", cumulative_explained_variance_manual[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4a6d3",
   "metadata": {},
   "source": [
    "SVD Results:\n",
    "Explained Variance Ratio (SVD):\t [0.36837609 0.08561444]\n",
    "Singular Values (SVD):\t\t\t [133109.6694485   26344.70205895]\n",
    "Cumulative Explained Variance (SVD):\t [0.36837609 0.45399053]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118f8c7",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "### Conclusion\n",
    "\n",
    "#### sklearn PCA and SVD Results:\n",
    "\n",
    "**PCA Results:**\n",
    "\n",
    "| Metric                                | PCA Values                             |\n",
    "|---------------------------------------|----------------------------------------|\n",
    "| Explained Variance Ratio (PCA)         | [0.4172533, 0.08504119]               |\n",
    "| Eigenvalues (PCA)                     | [1405229.4284625, 286402.49043666]    |\n",
    "| Cumulative Explained Variance (PCA)   | [0.4172533, 0.5022945]                |\n",
    "\n",
    "**SVD Results:**\n",
    "\n",
    "| Metric                                | SVD Values                             |\n",
    "|---------------------------------------|----------------------------------------|\n",
    "| Explained Variance Ratio (SVD)         | [0.36837609, 0.08561444]              |\n",
    "| Singular Values (SVD)                 | [133109.6694485, 26344.70205895]      |\n",
    "| Cumulative Explained Variance (SVD)   | [0.36837609, 0.45399053]              |\n",
    "\n",
    "#### Manual Method PCA and SVD Results:\n",
    "\n",
    "**Manual PCA Results:**\n",
    "\n",
    "| Metric                                | Manual PCA Values                     |\n",
    "|---------------------------------------|----------------------------------------|\n",
    "| Explained Variance Ratio              | [0.5591340500663632, 0.4408659499336368]|\n",
    "| Eigenvalues                           | [170926974.74379313, 134772480.91179776]|\n",
    "| Cumulative Explained Variance         | [0.5591340500663632, 1.0]             |\n",
    "\n",
    "**Manual SVD Results:**\n",
    "\n",
    "| Metric                                | Manual SVD Values                     |\n",
    "|---------------------------------------|----------------------------------------|\n",
    "| Explained Variance Ratio (SVD)         | [0.782912432611862, 0.041141589703449395]|\n",
    "| Singular Values (Manual SVD)          | [(23633.322046648704+0j), (5417.622156598941+0j)]|\n",
    "| Cumulative Explained Variance (SVD)   | [0.782912432611862, 0.8240540223153114]|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431b535",
   "metadata": {},
   "source": [
    "#### Possible reasons for why sklearn and my manual codes are not the same:\n",
    "\n",
    "Normalization and Scaling:\n",
    "\n",
    "In the domain of data preprocessing, Sklearn's PCA leverages an automated scaling mechanism to achieve feature standardization. Preceding the PCA computation, it systematically enforces a zero mean and unit variance for each feature. Notably, an oversight in explicitly normalizing data prior to PCA within my manual implementation has surfaced. This absence of normalization bears significance, as the resulting scaling factor holds the potential to exert a substantial impact on the final analytical outcomes.\n",
    "\n",
    "Algorithm Differences:\n",
    "\n",
    "Diverging into the intricacies of algorithmic implementations, Sklearn's PCA is distinguished by a sophisticated suite of algorithms and optimization techniques, effectively concealed beneath its operational facade. In contrast, the manual methodology at play may lack equivalent robustness. The intricacies of algorithmic processes and ancillary preprocessing steps, such as data centering, emerge as critical focal points where distinctions may manifest.\n",
    "\n",
    "Numerical Stability:\n",
    "\n",
    "The consideration of numerical stability assumes prominence in evaluating the methodologies employed. Evidently, my manual method may demonstrate diminished nimbleness compared to Sklearn, particularly in the realm of numerical handling. Analogous to a precarious tightrope act, the manual methodology exhibits a degree of instability, notably when confronting larger matrices. Recognizing the centrality of numerical stability becomes imperative, given the potential for seemingly inconsequential calculation errors to propagate into discernible disparities within the analytical results.\n",
    "\n",
    "Parameter Differences:\n",
    "\n",
    "In the sphere of parameterization, meticulous attention to alignment becomes imperative. Ensuring congruence in configuration settings between Sklearn and the manual implementation is foundational. Scrutiny of parameters, encompassing facets such as the number of components, tolerance levels, and latent hyperparameters, becomes a requisite. These parameters, acting as instrumental determinants, wield substantive influence over the ultimate analytical outcomes. Consequently, a judicious examination and synchronization of these settings become indispensable for achieving coherence in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d1b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
